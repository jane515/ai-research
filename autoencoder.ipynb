{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00123273",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pylab import rcParams\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import optimizers, Sequential\n",
    "from keras.models import Model\n",
    "from keras.utils import plot_model\n",
    "from keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten\n",
    "from keras.callbacks import History\n",
    "from keras import callbacks\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve\n",
    "from sklearn.metrics import recall_score, classification_report, auc, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support, f1_score\n",
    "\n",
    "from wtte.wtte import WeightWatcher\n",
    "import wtte.wtte as wtte\n",
    "import wtte.weibull as weibull\n",
    "\n",
    "from numpy.random import seed\n",
    "seed(7)\n",
    "from tensorflow import set_random_seed\n",
    "set_random_seed(11)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "SEED = 123 #used to help randomly select the data points\n",
    "DATA_SPLIT_PCT = 0.2\n",
    "np.random.seed(2)\n",
    "pd.set_option(\"display.max_rows\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3495a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_col = 'id'\n",
    "time_col = 'cycle'\n",
    "feature_cols = ['setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "column_names = [id_col, time_col] + feature_cols\n",
    "\n",
    "np.set_printoptions(suppress=True, threshold=10000)\n",
    "train_orig = pd.read_csv('train_FD001.txt', sep=\" \", header=None)\n",
    "train_orig.drop(train_orig.columns[[26, 27]], axis=1, inplace=True)\n",
    "train_orig.columns = column_names\n",
    "test_x_orig = pd.read_csv('test_FD001.txt', sep=\" \", header=None)\n",
    "test_x_orig.drop(test_x_orig.columns[[26, 27]], axis=1, inplace=True)\n",
    "test_x_orig.columns = column_names\n",
    "test_y_orig = pd.read_csv('RUL_FD001.txt', header=None, names=['T'])\n",
    "test_y_orig.columns=['rul']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c93367b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_orig.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276d6a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_orig.set_index(['id', 'cycle'], verify_integrity=True)\n",
    "train_orig.set_index(['id', 'cycle'], verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9023c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#minmaxScaler\n",
    "train_orig['cycle_norm'] = train_orig['cycle']\n",
    "cols_normalize = train_orig.columns.difference(['id','cycle','RUL'])\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "norm_train_df = pd.DataFrame(min_max_scaler.fit_transform(train_orig[cols_normalize]), \n",
    "                             columns=cols_normalize, \n",
    "                             index=train_orig.index)\n",
    "join_df = train_orig[train_orig.columns.difference(cols_normalize)].join(norm_train_df)\n",
    "train_orig = join_df.reindex(columns = train_orig.columns)\n",
    "train_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bf1051",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x_orig['cycle_norm'] = test_x_orig['cycle']\n",
    "norm_test_df = pd.DataFrame(min_max_scaler.fit_transform(test_x_orig[cols_normalize]), \n",
    "                            columns=cols_normalize, \n",
    "                            index=test_x_orig.index)\n",
    "test_join_df = test_x_orig[test_x_orig.columns.difference(cols_normalize)].join(norm_test_df)\n",
    "test_x_orig = test_join_df.reindex(columns = test_x_orig.columns)\n",
    "test_x_orig = test_x_orig.reset_index(drop=True)\n",
    "test_x_orig.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808f3d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# Combine the X values to normalize them, \n",
    "all_data_orig = pd.concat([train_orig, test_x_orig])\n",
    "# all_data = all_data[feature_cols]\n",
    "# all_data[feature_cols] = normalize(all_data[feature_cols].values)\n",
    "feature_cols = ['setting1', 'setting2', 'setting3', 's1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9', 's10', 's11', 's12', 's13', 's14', 's15', 's16', 's17', 's18', 's19', 's20', 's21']\n",
    "\n",
    "scaler=pipeline.Pipeline(steps=[\n",
    "#     ('z-scale', StandardScaler()),\n",
    "     ('minmax', MinMaxScaler(feature_range=(-1, 1))),\n",
    "     ('remove_constant', VarianceThreshold())\n",
    "])\n",
    "\n",
    "all_data = all_data_orig.copy()\n",
    "test_data = test_x_orig[test_x_orig['id'] == 13]\n",
    "\n",
    "all_data = np.concatenate([all_data_orig[['id', 'cycle']], scaler.fit_transform(all_data[feature_cols])], axis=1)\n",
    "test_data = np.concatenate([test_data[['id', 'cycle']], scaler.fit_transform(test_data[feature_cols])], axis=1)\n",
    "print(test_data.shape)\n",
    "print(type(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2297ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then split them back out\n",
    "train = all_data[0:train_orig.shape[0], :]\n",
    "test = all_data[train_orig.shape[0]:, :]\n",
    "\n",
    "# Make engine numbers and days zero-indexed, for everybody's sanity\n",
    "train[:, 0:2] -= 1\n",
    "test[:, 0:2] -= 1\n",
    "test_data[:,0:2] -= 1\n",
    "\n",
    "test_data.shape\n",
    "id_df = pd.DataFrame(train, columns = ['id', 'cycle'] + feature_cols[0:17])\n",
    "id_df['cycle'].shape\n",
    "\n",
    "id_test_df = pd.DataFrame(test, columns = ['id', 'cycle'] + feature_cols[0:17])\n",
    "id_test_df['cycle'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02050b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TODO: replace using wtte data pipeline routine\n",
    "def build_data(engine, time, x, max_time, is_test, mask_value, is_test_13):\n",
    "    # y[0] will be days remaining, y[1] will be event indicator, always 1 for this data\n",
    "    out_y = []\n",
    "    \n",
    "    # number of features\n",
    "    d = x.shape[1]\n",
    "\n",
    "    # A full history of sensor readings to date for each x\n",
    "    out_x = []\n",
    "    n_engines=100\n",
    "    max_engine_time = 0\n",
    "    \n",
    "    if(is_test_13):\n",
    "        for i in tqdm(range(n_engines)):\n",
    "            if(i == 12):\n",
    "                # When did the engine fail? (Last day + 1 for train data, irrelevant for test.)\n",
    "                max_engine_time = int(np.max(time[engine == i])) + 1\n",
    "\n",
    "\n",
    "                if is_test:\n",
    "                    start = max_engine_time - 1\n",
    "                else:\n",
    "                    start = 0\n",
    "\n",
    "                this_x = []\n",
    "\n",
    "                for j in range(start, max_engine_time):\n",
    "                    engine_x = x[engine == i]\n",
    "\n",
    "                    out_y.append(np.array((max_engine_time - j, 1), ndmin=2))\n",
    "\n",
    "                    xtemp = np.zeros((1, max_time, d))\n",
    "                    xtemp += mask_value\n",
    "        #             xtemp = np.full((1, max_time, d), mask_value)\n",
    "\n",
    "                    xtemp[:, max_time-min(j, 99)-1:max_time, :] = engine_x[max(0, j-max_time+1):j+1, :]\n",
    "                    this_x.append(xtemp)\n",
    "\n",
    "                this_x = np.concatenate(this_x)\n",
    "                out_x.append(this_x)\n",
    "    else:\n",
    "        for i in tqdm(range(n_engines)):\n",
    "            # When did the engine fail? (Last day + 1 for train data, irrelevant for test.)\n",
    "            max_engine_time = int(np.max(time[engine == i])) + 1\n",
    "\n",
    "\n",
    "            if is_test:\n",
    "                start = max_engine_time - 1\n",
    "            else:\n",
    "                start = 0\n",
    "\n",
    "            this_x = []\n",
    "\n",
    "            for j in range(start, max_engine_time):\n",
    "                engine_x = x[engine == i]\n",
    "\n",
    "                out_y.append(np.array((max_engine_time - j, 1), ndmin=2))\n",
    "\n",
    "                xtemp = np.zeros((1, max_time, d))\n",
    "                xtemp += mask_value\n",
    "    #             xtemp = np.full((1, max_time, d), mask_value)\n",
    "\n",
    "                xtemp[:, max_time-min(j, 99)-1:max_time, :] = engine_x[max(0, j-max_time+1):j+1, :]\n",
    "                this_x.append(xtemp)\n",
    "\n",
    "            this_x = np.concatenate(this_x)\n",
    "            out_x.append(this_x)\n",
    "    out_x = np.concatenate(out_x)\n",
    "    out_y = np.concatenate(out_y)\n",
    "    return out_x, out_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895c742e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configurable observation look-back period for each engine/day\n",
    "max_time = 100\n",
    "mask_value = -99\n",
    "\n",
    "train_x, train_y = build_data(engine=train[:, 0], time=train[:, 1], x=train[:, 2:], max_time=max_time, is_test=False, mask_value=mask_value, is_test_13=False)\n",
    "test_x,_ = build_data(engine=test[:, 0], time=test[:, 1], x=test[:, 2:], max_time=max_time, is_test=True, mask_value=mask_value, is_test_13=False)\n",
    "\n",
    "test_new,_ = build_data(engine=test_data[:, 0], time=test_data[:, 1], x=test_data[:, 2:], max_time=max_time, is_test=True, mask_value=mask_value, is_test_13=True)\n",
    "\n",
    "train_x_2, train_y_2 = build_data(engine=test[:, 0], time=test[:, 1], x=test[:, 2:], max_time=max_time, is_test=False, mask_value=mask_value, is_test_13=False)\n",
    "train_x_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91f1295",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test_y_orig.copy()\n",
    "print('train_x', train_x.shape, 'train_y', train_y.shape, 'test_x', test_x.shape, 'test_y', test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f11702",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps =  train_x.shape[1]\n",
    "n_features =  train_x.shape[2]\n",
    "\n",
    "epochs = 100\n",
    "batch = 100\n",
    "lr = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9516d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshape the data\n",
    "train_x = train_x.reshape(train_x.shape[0], timesteps, n_features)\n",
    "test_x = test_x.reshape(test_x.shape[0], timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a416cbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(X):\n",
    "    '''\n",
    "    Flatten a 3D array.\n",
    "    \n",
    "    Input\n",
    "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    \n",
    "    Output\n",
    "    flattened_X  A 2D array, sample x features.\n",
    "    '''\n",
    "    flattened_X = np.empty((X.shape[0], X.shape[2]))  # sample x features array.\n",
    "    for i in range(X.shape[0]):\n",
    "        flattened_X[i] = X[i, (X.shape[1]-1), :]\n",
    "    return(flattened_X)\n",
    "\n",
    "def scale(X, scaler):\n",
    "    '''\n",
    "    Scale 3D array.\n",
    "\n",
    "    Inputs\n",
    "    X            A 3D array for lstm, where the array is sample x timesteps x features.\n",
    "    scaler       A scaler object, e.g., sklearn.preprocessing.StandardScaler, sklearn.preprocessing.normalize\n",
    "    \n",
    "    Output\n",
    "    X            Scaled 3D array.\n",
    "    '''\n",
    "    for i in range(X.shape[0]):\n",
    "        X[i, :, :] = scaler.transform(X[i, :, :])\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2295ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(timesteps)\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc6a561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a scaler using the training data.\n",
    "scaler = StandardScaler().fit(flatten(train_x))\n",
    "X_train_scaled = scale(train_x, scaler)\n",
    "X_test_scaled = scale(test_x, scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8a9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.set_epsilon(1e-10)\n",
    "print('epsilon', K.epsilon())\n",
    "\n",
    "\n",
    "encoder_decoder = Sequential()\n",
    "encoder_decoder.add(LSTM(10, activation='relu', input_shape=(timesteps, n_features), return_sequences=True))\n",
    "encoder_decoder.add(LSTM(6, activation='relu', return_sequences=True))\n",
    "encoder_decoder.add(LSTM(1, activation='relu'))\n",
    "encoder_decoder.add(RepeatVector(timesteps))\n",
    "encoder_decoder.add(LSTM(10, activation='relu', return_sequences=True))\n",
    "encoder_decoder.add(LSTM(1, activation='relu', return_sequences=True))\n",
    "encoder_decoder.add(TimeDistributed(Dense(17)))\n",
    "encoder_decoder.summary()\n",
    "\n",
    "adam = optimizers.Adam(lr)\n",
    "\n",
    "loss = wtte.loss(kind='discrete',reduce_loss=False).loss_function\n",
    "encoder_decoder.compile(loss=\"mse\", optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ecfbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# history = History()\n",
    "# weightwatcher = WeightWatcher()\n",
    "# nanterminator = callbacks.TerminateOnNaN()\n",
    "\n",
    "# lstm_autoencoder.fit(train_x, train_x, \n",
    "#                     epochs=epochs, \n",
    "#                     batch_size=batch, \n",
    "#                     validation_data=(test_x, test_x),\n",
    "#                     verbose=1,\n",
    "#                     callbacks=[nanterminator,history,weightwatcher])\n",
    "\n",
    "encoder_decoder_history = encoder_decoder.fit(X_train_scaled, X_train_scaled, \n",
    "                                              batch_size=batch, \n",
    "                                              epochs=epochs, \n",
    "                                              validation_data=(X_test_scaled, X_test_scaled),\n",
    "                                              verbose=2).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f571420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(lstm_autoencoder_history['loss'], linewidth=2, label='Train')\n",
    "# plt.plot(lstm_autoencoder_history['val_loss'], linewidth=2, label='Valid')\n",
    "# plt.legend(loc='upper right')\n",
    "# plt.title('Model loss')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.show()\n",
    "\n",
    "rpt_vector_layer = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[3].output)\n",
    "time_dist_layer = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[5].output)\n",
    "encoder_decoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be430c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "rpt_vector_layer_output = rpt_vector_layer.predict(train_x[:1])\n",
    "print('Repeat vector output shape', rpt_vector_layer_output.shape)\n",
    "print('Repeat vector output sample')\n",
    "print(rpt_vector_layer_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67d6878",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_dist_layer_output = time_dist_layer.predict(train_x[:1])\n",
    "print('Time distributed output shape', time_dist_layer_output.shape)\n",
    "print('Time distributed output sample')\n",
    "print(time_dist_layer_output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65f194",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b227f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = encoder.predict(train_x)\n",
    "train_encoded_2nd = encoder.predict(train_x)\n",
    "# validation_encoded = encoder.predict(X_valid)\n",
    "print('Encoded time-series shape', train_encoded.shape)\n",
    "print('Encoded time-series sample', train_encoded[0])\n",
    "\n",
    "for i in train_encoded:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc64b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c30c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(inputs=encoder_decoder.inputs, outputs=encoder_decoder.layers[4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42121c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded = autoencoder.predict(train_x)\n",
    "for i in train_encoded:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bdb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = encoder.predict(train_x_2)\n",
    "test_encoded_2nd = encoder.predict(train_x_2)\n",
    "test_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5366c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_encoded.shape\n",
    "test_encoded.shape\n",
    "df2 = pd.DataFrame(test_encoded, columns = ['s'])\n",
    "df2.insert(0,'id',id_test_df['id'])\n",
    "df2.insert(1,'cycle',id_test_df['cycle'])\n",
    "# df2.insert(3, 's2', test_encoded_2nd)\n",
    "\n",
    "df = pd.DataFrame(train_encoded, columns = ['s'])\n",
    "df.insert(0,'id',id_df['id'])\n",
    "df.insert(1,'cycle',id_df['cycle'])\n",
    "# df['id'].append(id_df['id'])\n",
    "# df['cycle'].append(id_df['cycle'])\n",
    "# df.insert(3, 's2', train_encoded_2nd)\n",
    "train_encoded_2 = df.to_numpy()\n",
    "print(train_encoded_2.shape)\n",
    "\n",
    "test_encoded_2 = df2.to_numpy()\n",
    "# test_encoded_2.shape\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60717c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configurable observation look-back period for each engine/day\n",
    "max_time = 100\n",
    "mask_value = -99\n",
    "\n",
    "train_x, train_y = build_data(engine=train_encoded_2[:, 0], time=train_encoded_2[:, 1], x=train_encoded_2[:, 2:], max_time=max_time, is_test=False, mask_value=mask_value, is_test_13=False)\n",
    "test_x,_ = build_data(engine=test_encoded_2[:, 0], time=test_encoded_2[:, 1], x=test_encoded_2[:, 2:], max_time=max_time, is_test=True, mask_value=mask_value, is_test_13=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745dbc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e767d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tte_mean_train = np.nanmean(train_y[:,0])\n",
    "mean_u = np.nanmean(train_y[:,1])\n",
    "\n",
    "# Initialization value for alpha-bias \n",
    "init_alpha = -1.0/np.log(1.0-1.0/(tte_mean_train+1.0) )\n",
    "init_alpha = init_alpha/mean_u\n",
    "print('tte_mean_train', tte_mean_train, 'init_alpha: ',init_alpha,'mean uncensored train: ',mean_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe72f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.layers import Masking\n",
    "from keras.layers import LSTM, GRU\n",
    "from keras.layers import Lambda\n",
    "\n",
    "K.set_epsilon(1e-10)\n",
    "print('epsilon', K.epsilon())\n",
    "\n",
    "history = History()\n",
    "weightwatcher = WeightWatcher()\n",
    "nanterminator = callbacks.TerminateOnNaN()\n",
    "# reduce_lr = callbacks.ReduceLROnPlateau(monitor='loss', \n",
    "#                                         factor=0.5, \n",
    "#                                         patience=50, \n",
    "#                                         verbose=0, \n",
    "#                                         mode='auto', \n",
    "#                                         epsilon=0.0001, \n",
    "#                                         cooldown=0, \n",
    "#                                         min_lr=1e-8)\n",
    "\n",
    "n_features = train_x.shape[-1]\n",
    "print(n_features)\n",
    "\n",
    "# Start building our model\n",
    "model = Sequential()\n",
    "\n",
    "# Mask parts of the lookback period that are all zeros (i.e., unobserved) so they don't skew the model\n",
    "model.add(Masking(mask_value=mask_value, input_shape=(None, 2)))\n",
    "\n",
    "# model.add(BatchNormalization())\n",
    "\n",
    "# LSTM is just a common type of RNN. You could also try anything else (e.g., GRU).\n",
    "model.add(GRU(20, activation='tanh', recurrent_dropout=0.25))\n",
    "\n",
    "# model.add(Dense(20))\n",
    "\n",
    "# We need 2 neurons to output Alpha and Beta parameters for our Weibull distribution\n",
    "model.add(Dense(2))\n",
    "\n",
    "# Apply the custom activation function mentioned above\n",
    "# model.add(Activation(activate))\n",
    "\n",
    "model.add(Lambda(wtte.output_lambda, \n",
    "                 arguments={\"init_alpha\":init_alpha, \n",
    "                            \"max_beta_value\":100.0, \n",
    "                            \"alpha_kernel_scalefactor\":0.5\n",
    "                           },\n",
    "                ))\n",
    "\n",
    "# Use the discrete log-likelihood for Weibull survival data as our loss function\n",
    "loss = wtte.loss(kind='discrete',reduce_loss=False).loss_function\n",
    "\n",
    "model.compile(loss=loss, optimizer=keras.optimizers.Adam(lr=.01, clipvalue=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2945fe90",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eb6150",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.fit(train_x, train_y,\n",
    "          epochs=1,\n",
    "          batch_size=100, \n",
    "          verbose=1,\n",
    "          validation_data=(test_x, test_y),\n",
    "          callbacks=[nanterminator,history,weightwatcher])\n",
    "except:\n",
    "    print(\"\\nError - repeating\")\n",
    "\n",
    "    model.compile(loss=loss, optimizer=keras.optimizers.Adam(lr=.01, clipvalue=0.5))\n",
    "\n",
    "    model.fit(train_x, train_y,\n",
    "          epochs=1,\n",
    "          batch_size=100, \n",
    "          verbose=1,\n",
    "          validation_data=(test_x, test_y),\n",
    "          callbacks=[nanterminator,history,weightwatcher])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae32cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965c8715",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
